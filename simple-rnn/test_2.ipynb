{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: sentences and their labels (1 for positive, 0 for negative)\n",
    "sentences = [\n",
    "    'I love this movie',\n",
    "    'This film was terrible',\n",
    "    'Absolutely fantastic experience',\n",
    "    'Worst movie ever',\n",
    "    'I really enjoyed it',\n",
    "    'Not my favorite',\n",
    "    'Amazing storyline and characters',\n",
    "    'I hated it',\n",
    "    'Best film I have seen',\n",
    "    'Awful and boring',\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Step 1: Tokenize the sentences\n",
    "vocab_size = 1000  # This limits the number of tokens the tokenizer will keep\n",
    "max_length = 10  # Maximum length of the sequences\n",
    "\n",
    "# Initialize the tokenizer and fit on sentences\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'i': 2,\n",
       " 'this': 3,\n",
       " 'movie': 4,\n",
       " 'film': 5,\n",
       " 'it': 6,\n",
       " 'and': 7,\n",
       " 'love': 8,\n",
       " 'was': 9,\n",
       " 'terrible': 10,\n",
       " 'absolutely': 11,\n",
       " 'fantastic': 12,\n",
       " 'experience': 13,\n",
       " 'worst': 14,\n",
       " 'ever': 15,\n",
       " 'really': 16,\n",
       " 'enjoyed': 17,\n",
       " 'not': 18,\n",
       " 'my': 19,\n",
       " 'favorite': 20,\n",
       " 'amazing': 21,\n",
       " 'storyline': 22,\n",
       " 'characters': 23,\n",
       " 'hated': 24,\n",
       " 'best': 25,\n",
       " 'have': 26,\n",
       " 'seen': 27,\n",
       " 'awful': 28,\n",
       " 'boring': 29}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Pad the sequences to ensure uniform input length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 - 5s - loss: 0.6924 - accuracy: 0.7000 - 5s/epoch - 5s/step\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 0.6819 - accuracy: 0.8000 - 15ms/epoch - 15ms/step\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 0.6716 - accuracy: 0.9000 - 18ms/epoch - 18ms/step\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 0.6613 - accuracy: 0.9000 - 16ms/epoch - 16ms/step\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 0.6507 - accuracy: 0.9000 - 19ms/epoch - 19ms/step\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 0.6396 - accuracy: 0.9000 - 19ms/epoch - 19ms/step\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 0.6280 - accuracy: 0.9000 - 17ms/epoch - 17ms/step\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 0.6154 - accuracy: 0.9000 - 25ms/epoch - 25ms/step\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 0.6019 - accuracy: 0.9000 - 17ms/epoch - 17ms/step\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 0.5872 - accuracy: 0.9000 - 21ms/epoch - 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21a6784cad0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Create the RNN model\n",
    "embedding_dim = 16  # Size of the word embeddings\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    SimpleRNN(units=32, return_sequences=False),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the model\n",
    "labels = np.array(labels)\n",
    "model.fit(padded_sequences, labels, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 216ms/step\n",
      "Sentence: \"I loved the acting\" - Sentiment: Positive\n",
      "Sentence: \"The plot was dull and predictable\" - Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Evaluate the model with a new sentence\n",
    "new_sentences = ['I loved the acting', 'The plot was dull and predictable']\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Get the model's predictions for new data\n",
    "predictions = model.predict(new_padded)\n",
    "for i, sentence in enumerate(new_sentences):\n",
    "    sentiment = 'Positive' if predictions[i] > 0.5 else 'Negative'\n",
    "    print(f'Sentence: \"{sentence}\" - Sentiment: {sentiment}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
