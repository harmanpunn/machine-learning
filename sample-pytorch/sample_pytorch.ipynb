{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Normalization:\n",
    "Both `transform_train` and `transform_test` include a `transforms.Normalize` step, which is used to normalize the pixel values of images. For the CIFAR-10 dataset, this typically involves subtracting the mean and dividing by the standard deviation of each channel (R, G, B).\n",
    "\n",
    "This normalization helps in stabilizing the training process by ensuring that the input features have a consistent scale, which in turn can speed up convergence and improve model performance.\n",
    "\n",
    "## 2. Data Augmentation (transform_train only):\n",
    "The `transform_train` includes additional transformations like `RandomHorizontalFlip` and `RandomCrop`, which are examples of data augmentation techniques. Data augmentation is used only on the training set to artificially increase the diversity of the training data, thereby reducing overfitting.\n",
    "\n",
    "- **RandomHorizontalFlip**: This randomly flips the image horizontally with a probability of 50%, helping the model learn features invariant to horizontal orientation.\n",
    "- **RandomCrop**: This randomly crops a part of the image and resizes it to the original dimensions, which helps the model become robust to spatial translations and learn better features.\n",
    "\n",
    "## 3. Converting to Tensor:\n",
    "Both `transform_train` and `transform_test` include the `transforms.ToTensor()` transformation, which converts the images from PIL format to PyTorch tensors. This is necessary because PyTorch models expect inputs as tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:11<00:00, 15377369.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for the training and validation datasets\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create DataLoader for both training and testing datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conv2d (Convolutional Layer)\n",
    "- **Purpose**: Extracts features from the input image, like edges, textures, and patterns.\n",
    "- **Operation**: Applies multiple filters (kernels) to the input image, producing feature maps. Each filter focuses on different aspects of the image.\n",
    "- **Output**: A set of feature maps highlighting different features detected by the filters.\n",
    "\n",
    "## 2. MaxPool2d (Max Pooling Layer)\n",
    "- **Purpose**: Reduces the spatial dimensions (height and width) of the feature maps.\n",
    "- **Operation**: Takes the maximum value from small regions (e.g., 2x2) of the feature maps, which helps in downsampling and making the network more computationally efficient.\n",
    "- **Output**: Smaller, more manageable feature maps that retain the most important information.\n",
    "\n",
    "## 3. Linear (Fully Connected Layer)\n",
    "- **Purpose**: Combines the features extracted by the convolutional layers to make final predictions.\n",
    "- **Operation**: Takes the flattened output from the previous layers and maps it to the desired output classes. Each neuron in the layer is connected to all the outputs of the previous layer.\n",
    "- **Output**: Final scores (logits) for each class in the classification task.\n",
    "\n",
    "## 4. Forward Pass\n",
    "- **Purpose**: Defines the flow of data through the network, specifying how the input transforms into the output step-by-step.\n",
    "- **Operation**:\n",
    "  - **Convolution + ReLU + Pooling**: Extracts and condenses features.\n",
    "  - **Flattening**: Converts the 2D feature maps into a 1D vector.\n",
    "  - **Fully Connected Layers**: Combines features to classify the input.\n",
    "- **Output**: A vector representing the predicted class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.994\n",
      "[1, 200] loss: 1.701\n",
      "[1, 300] loss: 1.546\n",
      "[1, 400] loss: 1.485\n",
      "[1, 500] loss: 1.448\n",
      "[1, 600] loss: 1.358\n",
      "[1, 700] loss: 1.301\n",
      "[2, 100] loss: 1.241\n",
      "[2, 200] loss: 1.204\n",
      "[2, 300] loss: 1.179\n",
      "[2, 400] loss: 1.129\n",
      "[2, 500] loss: 1.089\n",
      "[2, 600] loss: 1.064\n",
      "[2, 700] loss: 1.053\n",
      "[3, 100] loss: 1.007\n",
      "[3, 200] loss: 0.984\n",
      "[3, 300] loss: 0.949\n",
      "[3, 400] loss: 0.930\n",
      "[3, 500] loss: 0.914\n",
      "[3, 600] loss: 0.921\n",
      "[3, 700] loss: 0.927\n",
      "[4, 100] loss: 0.855\n",
      "[4, 200] loss: 0.857\n",
      "[4, 300] loss: 0.856\n",
      "[4, 400] loss: 0.866\n",
      "[4, 500] loss: 0.820\n",
      "[4, 600] loss: 0.837\n",
      "[4, 700] loss: 0.813\n",
      "[5, 100] loss: 0.790\n",
      "[5, 200] loss: 0.774\n",
      "[5, 300] loss: 0.756\n",
      "[5, 400] loss: 0.783\n",
      "[5, 500] loss: 0.778\n",
      "[5, 600] loss: 0.766\n",
      "[5, 700] loss: 0.756\n",
      "[6, 100] loss: 0.726\n",
      "[6, 200] loss: 0.741\n",
      "[6, 300] loss: 0.770\n",
      "[6, 400] loss: 0.723\n",
      "[6, 500] loss: 0.717\n",
      "[6, 600] loss: 0.701\n",
      "[6, 700] loss: 0.704\n",
      "[7, 100] loss: 0.686\n",
      "[7, 200] loss: 0.685\n",
      "[7, 300] loss: 0.663\n",
      "[7, 400] loss: 0.684\n",
      "[7, 500] loss: 0.665\n",
      "[7, 600] loss: 0.708\n",
      "[7, 700] loss: 0.672\n",
      "[8, 100] loss: 0.652\n",
      "[8, 200] loss: 0.650\n",
      "[8, 300] loss: 0.663\n",
      "[8, 400] loss: 0.686\n",
      "[8, 500] loss: 0.660\n",
      "[8, 600] loss: 0.651\n",
      "[8, 700] loss: 0.645\n",
      "[9, 100] loss: 0.627\n",
      "[9, 200] loss: 0.624\n",
      "[9, 300] loss: 0.608\n",
      "[9, 400] loss: 0.653\n",
      "[9, 500] loss: 0.636\n",
      "[9, 600] loss: 0.611\n",
      "[9, 700] loss: 0.628\n",
      "[10, 100] loss: 0.582\n",
      "[10, 200] loss: 0.632\n",
      "[10, 300] loss: 0.628\n",
      "[10, 400] loss: 0.584\n",
      "[10, 500] loss: 0.600\n",
      "[10, 600] loss: 0.630\n",
      "[10, 700] loss: 0.587\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 78.76%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# Example: Inference on a single image\n",
    "from PIL import Image\n",
    "\n",
    "def predict(model, image_path):\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# Example usage\n",
    "image_path = '1.jpg'\n",
    "predicted_class = predict(model, image_path)\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsingh\\AppData\\Local\\Temp\\ipykernel_35068\\4183602993.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('simple_cnn.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'simple_cnn.pth')\n",
    "\n",
    "# Load the model\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('simple_cnn.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
