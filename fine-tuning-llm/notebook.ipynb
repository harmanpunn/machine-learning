{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = 'data/Resume_Harmanpreet.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    pdf_reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "text = read_pdf(pdf_file)\n",
    "print(\"Document Loaded Successfully!\")\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 12 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(f\"Document split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Generate Embeddings\n",
    "## We use a pre-trained Sentence Transformer model to convert text chunks into numerical embeddings.\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and vector store created. <langchain_community.vectorstores.faiss.FAISS object at 0x0000024F9C0F1E50>\n"
     ]
    }
   ],
   "source": [
    "# Create a Vector Store (FAISS)\n",
    "## We store the embeddings in a FAISS vector store for efficient similarity search.\n",
    "\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "print(\"Embeddings and vector store created.\", vectorstore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hsingh\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load a Language Model (LLM)\n",
    "## We load a pre-trained language model from HuggingFace. For demonstration purposes, we'll use a smaller model like gpt2 to ensure it runs smoothly on most machines.\n",
    "print(\"Loading language model...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_id = 'gpt2'\n",
    "model_id = 'google/flan-t5-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set up the text generation pipeline with max_new_tokens to prevent errors\n",
    "generation_pipeline = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device == 'cuda' else -1,\n",
    "    max_new_tokens=150,  # Adjust as needed\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "print(\"Language model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set Up the RetrievalQA Chain\n",
    "## We set up a RetrievalQA chain using LangChain. This chain will handle retrieving relevant documents and generating answers.\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# prompt_template = \"\"\"Answer the following question based on the context below.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# Answer:\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"You are an AI assistant tasked with answering the question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Please provide a concise and accurate answer based solely on the context.\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "qa = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=qa_chain,\n",
    "    return_source_documents=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RetrievalQA chain\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"map_reduce\",  # Using \"map_reduce\" to handle longer documents\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "# qa = RetrievalQA(\n",
    "#     retriever=retriever,\n",
    "#     combine_documents_chain=llm_chain,\n",
    "#     return_source_documents=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ask Questions and Generate Answers\n",
    "# def ask_question(query):\n",
    "#     print(\"Generating answer...\")\n",
    "\n",
    "#     max_tokens = 1024\n",
    "#     tokens = tokenizer(query, return_tensors=\"pt\", max_length=max_tokens, truncation=True).input_ids\n",
    "\n",
    "#     try:\n",
    "#         answer = qa.run(query)\n",
    "#         print(\"Answer:\", answer)\n",
    "#     except Exception as e:\n",
    "#         print(\"Error:\", e)\n",
    "\n",
    "def ask_question(query):\n",
    "    print(f\"Question: {query}\")\n",
    "    print(\"Generating answer...\")\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Print the retrieved documents for debugging\n",
    "    # print(\"\\n--- Retrieved Documents ---\")\n",
    "    # # for doc in retrieved_docs:\n",
    "    # #     print(doc.page_content)\n",
    "\n",
    "    # Combine the retrieved documents into context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Determine the maximum input length\n",
    "    max_input_tokens = tokenizer.model_max_length - 50  # Reserve tokens for the answer and special tokens\n",
    "\n",
    "    # Tokenize context and question\n",
    "    input_ids = tokenizer.encode(context + \" \" + query, return_tensors='pt').to(device)\n",
    "    input_length = input_ids.shape[1]\n",
    "\n",
    "    if input_length > max_input_tokens:\n",
    "        # Truncate the context to fit within max_input_tokens\n",
    "        print(f\"Context is too long ({input_length} tokens), truncating...\")\n",
    "        # Calculate number of tokens to keep\n",
    "        tokens_to_keep = max_input_tokens - len(tokenizer.encode(query, return_tensors='pt').to(device)[0])\n",
    "        # Truncate context\n",
    "        context_ids = tokenizer.encode(context, return_tensors='pt').to(device)[0][:tokens_to_keep]\n",
    "        context = tokenizer.decode(context_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Prepare inputs for the chain\n",
    "    inputs = {\"context\": context, \"query\": query}\n",
    "\n",
    "    # Run the chain\n",
    "    answer = qa.run(inputs)\n",
    "    print(\"Answer:\")\n",
    "    print(answer)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the university name?\n",
      "Generating answer...\n",
      "Context is too long (711 tokens), truncating...\n",
      "Answer:\n",
      "Rutgers\n",
      "--------------------------------------------------\n",
      "Question: What programming languages am I proficient in?\n",
      "Generating answer...\n",
      "Context is too long (742 tokens), truncating...\n",
      "Answer:\n",
      "java, javascript, python, c++, groovy , html/css\n",
      "--------------------------------------------------\n",
      "Question: Describe my work experience related to machine learning.\n",
      "Generating answer...\n",
      "Context is too long (653 tokens), truncating...\n",
      "Answer:\n",
      "developed a language learning web application using next.js and fastapi , with aws services for storage and deployment.\n",
      "--------------------------------------------------\n",
      "Question: What are my educational qualifications?\n",
      "Generating answer...\n",
      "Context is too long (711 tokens), truncating...\n",
      "Answer:\n",
      "masters in computer science\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is the university name?\",\n",
    "    \"What programming languages am I proficient in?\",\n",
    "    \"Describe my work experience related to machine learning.\",\n",
    "    \"What are my educational qualifications?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    ask_question(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the university name?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the university name?\n",
      "Generating answer...\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "• comprehensive analysis of enrollment and performance trends over 5 years  in rutgers' foundational computer science courses \n",
      "using advanced data analytics and visualization tools.  \n",
      "• research on gender -based disparities in computer science enrollment and performance, utilizing data science to promote equity \n",
      "and inclusion in higher education.  \n",
      " \n",
      " \n",
      "academic projects  \n",
      " \n",
      "insightwing: ai -driven web content summarizer         link \n",
      "• developed a chrome extension utilizing falconllm and langchain  for efficient 60 -word web content summarization . \n",
      "• user-friendly interface with html/css and javascript and integrated a chat feature for interactive content engagement . \n",
      " \n",
      "global socioeconomic patterns and risk factors in suicide trends       link \n",
      "• analyzed the impact of gdp on suicide rates globally using r, revealing key economic correlations . \n",
      "• examined age and gender factors affecting suicide, providing insights through data visualizations .\n",
      "harmanpreet singh  \n",
      "+1 (848) -313-6708 • harmanpunn@gmail.com  • linkedin.com/in/harmanpunn  \n",
      " \n",
      "education  \n",
      " \n",
      "masters in computer science  \n",
      "rutgers, the state university of new jersey • new brunswick, nj • 2024 • 4.0  \n",
      " \n",
      "bachelors of technology in electronics and communication  \n",
      "dr. b.r ambedkar national institute of technology • jalandhar, india • 2017  \n",
      " \n",
      "skills  \n",
      " \n",
      "languages : java, javascript,  python,  c++, groovy , html/css  \n",
      "tools & frameworks : spring boot, maven, jenkins, aem, pandas, numpy, react.js, next .js, git , node.js, fastapi , flask, graphql  \n",
      "cloud & databases : sql,  nosql,  mongodb, aws (s3, redshift, quicksight, ec2, lambda, ecr, ecs, sqs , dynamodb ) \n",
      "engineering : docker,  kubernetes , hadoop, spark, microservices, unit testing , terraform, ci/cd  \n",
      " \n",
      "professional experience  \n",
      " \n",
      "invidi technologies  \n",
      "software engineer - data                     june 2024 – present, princeton, nj\n",
      "counties  and deployed the complete model on aws for streamlined data processing.  \n",
      "• analysis and identification of news deserts by mapping and monitoring geographical coverage in local news . \n",
      "  \n",
      "full stack developer and machine learning engineer , grid                                                  march 2023 – may 2023, nj  \n",
      "• developed a language learning web application using next.js  and fastapi , with aws services for storage and deployment.  \n",
      "• led ui/ux design, database schema, and integrated a ml model for real -time emotion detection with 72.4% accuracy . \n",
      " \n",
      "research assistant , dr. ana paula centeno                                                    november 2022 – february 2023, nj  \n",
      "data -driven analysis of cs enrollment and performance trends  \n",
      "• comprehensive analysis of enrollment and performance trends over 5 years  in rutgers' foundational computer science courses \n",
      "using advanced data analytics and visualization tools.\n",
      "Context is too long (711 tokens), truncating...\n",
      "Answer:\n",
      "state university of new jersey\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ask_question(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer science'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('what is rutgers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved: 3\n",
      "\n",
      "Document 1 content:\n",
      "• comprehensive analysis of enrollment and performance trends over 5 years  in rutgers' foundational computer science courses \n",
      "using advanced data analytics and visualization tools.  \n",
      "• research on gender -based disparities in computer science enrollment and performance, utilizing data science to promote equity \n",
      "and inclusion in higher education.  \n",
      " \n",
      " \n",
      "academic projects  \n",
      " \n",
      "insightwing: ai -driven web content summarizer         link \n",
      "• developed a chrome extension utilizing falconllm and langchain  for efficient 60 -word web content summarization . \n",
      "• user-friendly interface with html/css and javascript and integrated a chat feature for interactive content engagement . \n",
      " \n",
      "global socioeconomic patterns and risk factors in suicide trends       link \n",
      "• analyzed the impact of gdp on suicide rates globally using r, revealing key economic correlations . \n",
      "• examined age and gender factors affecting suicide, providing insights through data visualizations .\n",
      "\n",
      "\n",
      "Document 2 content:\n",
      "harmanpreet singh  \n",
      "+1 (848) -313-6708 • harmanpunn@gmail.com  • linkedin.com/in/harmanpunn  \n",
      " \n",
      "education  \n",
      " \n",
      "masters in computer science  \n",
      "rutgers, the state university of new jersey • new brunswick, nj • 2024 • 4.0  \n",
      " \n",
      "bachelors of technology in electronics and communication  \n",
      "dr. b.r ambedkar national institute of technology • jalandhar, india • 2017  \n",
      " \n",
      "skills  \n",
      " \n",
      "languages : java, javascript,  python,  c++, groovy , html/css  \n",
      "tools & frameworks : spring boot, maven, jenkins, aem, pandas, numpy, react.js, next .js, git , node.js, fastapi , flask, graphql  \n",
      "cloud & databases : sql,  nosql,  mongodb, aws (s3, redshift, quicksight, ec2, lambda, ecr, ecs, sqs , dynamodb ) \n",
      "engineering : docker,  kubernetes , hadoop, spark, microservices, unit testing , terraform, ci/cd  \n",
      " \n",
      "professional experience  \n",
      " \n",
      "invidi technologies  \n",
      "software engineer - data                     june 2024 – present, princeton, nj\n",
      "\n",
      "\n",
      "Document 3 content:\n",
      "counties  and deployed the complete model on aws for streamlined data processing.  \n",
      "• analysis and identification of news deserts by mapping and monitoring geographical coverage in local news . \n",
      "  \n",
      "full stack developer and machine learning engineer , grid                                                  march 2023 – may 2023, nj  \n",
      "• developed a language learning web application using next.js  and fastapi , with aws services for storage and deployment.  \n",
      "• led ui/ux design, database schema, and integrated a ml model for real -time emotion detection with 72.4% accuracy . \n",
      " \n",
      "research assistant , dr. ana paula centeno                                                    november 2022 – february 2023, nj  \n",
      "data -driven analysis of cs enrollment and performance trends  \n",
      "• comprehensive analysis of enrollment and performance trends over 5 years  in rutgers' foundational computer science courses \n",
      "using advanced data analytics and visualization tools.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsingh\\AppData\\Local\\Temp\\ipykernel_24544\\2408117288.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve relevant documents\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "print(f\"Number of documents retrieved: {len(retrieved_docs)}\")\n",
    "for idx, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {idx+1} content:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "summary = summarization_chain.run(retrieved_docs)\n",
    "\n",
    "content = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What programming languages is the person proficient in?\",\n",
    "    \"Describe the work experience related to machine learning.\",\n",
    "    \"What educational qualifications does the person have?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Question: {query}\")\n",
    "    ask_question(query)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLMChain' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create a new RetrievalQA chain with the custom llm_chain\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Now run the query\u001b[39;00m\n\u001b[0;32m     26\u001b[0m answer \u001b[38;5;241m=\u001b[39m qa\u001b[38;5;241m.\u001b[39mrun(query)\n",
      "File \u001b[1;32mc:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:213\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     emit_warning()\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:213\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     emit_warning()\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.raise_callback_manager_deprecation\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;129m@model_validator\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_callback_manager_deprecation\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise deprecation warning if callback_manager is used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both callback_manager and callbacks. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback_manager is deprecated, callbacks is the preferred \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter to pass in.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\hsingh\\OneDrive - INVIDI Technologies Corp\\machine learning\\machine-learning\\myenv\\Lib\\site-packages\\pydantic\\main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LLMChain' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a new prompt template that includes truncation logic\n",
    "prompt_template = \"\"\"Use the following context to answer the question.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Create a new RetrievalQA chain with the custom llm_chain\n",
    "qa = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=llm_chain,\n",
    ")\n",
    "\n",
    "# Now run the query\n",
    "answer = qa.run(query)\n",
    "print(\"Answer:\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
